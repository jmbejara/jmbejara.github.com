from mpi4py import MPIimport numpycomm = MPI.COMM_WORLDdef dotParallel(x,y):	rank = comm.Get_rank()		#test for conformability	print "rank", comm.rank, "and size", comm.size	if (x.size != y.size):		quit("vector length mismatch")		#currently, our program cannot handle sizes that are not evenly divided by	#the number of processors	n = x.size	p = comm.Get_size()	if(n % p != 0):		quit("the number of processors must evenly divide n.")		#length of each process's portion of the original vector	n_bar = n/p		#initialize as numpy arrays	dot = numpy.array([0.])		local_x = numpy.zeros(n_bar)	local_y = numpy.zeros(n_bar)			#divide up vectors	comm.Scatter(x, local_x, root=0)	comm.Scatter(y, local_y, root=0)	print "local_x", local_x, "from process", comm.rank		#local computation of dot product	local_dot = numpy.array([numpy.dot(local_x, local_y)])		print "local_dot", local_dot, "from process", comm.rank	#sum the results of each 	comm.Reduce(local_dot, dot, op = MPI.SUM)	print "hello2"	return dot[0]def example():	rank = comm.Get_rank()	#arbitrary example vectors, generated to be evenly divided by the number of 	#processes for convenience	p = comm.Get_size()	x = numpy.linspace(0,10,p*10) if comm.rank == 0 else None	y = numpy.linspace(20,30,p*10) if comm.rank == 0 else None	dot = dotParallel(x,y)		if (rank == 0):		print "The dot product is", dot		print "the real one", numpy.dot(x,y)if __name__ == "__main__":	example()